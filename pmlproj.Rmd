---
title: "HAR DATA prediction exercise using parsimonious GBM"
author: "Edwin Seah"
date: "23 Jul 2015"
output:
  html_document: default
  pdf_document: default
fontsize: 10pt
---
```{r global_options, include=FALSE}
# Setting global options for R markdown
knitr::opts_chunk$set(fig.path='figures/',
                      echo=TRUE, 
                      warning=FALSE,
                      message=FALSE)
```

### Executive Summary

The HAR dataset comprises Human Activity Recognition data regarding a series of five weighlifting activities performed by six research subjects. The project goal is to predict the activity performed, denoted by the **classe** variable, using a simple machine learning algorithm based on the recorded measurements. The approach aims to favour simplicity (fewer features) over maximizing marginal accuracy improvements, and makes use of the *caret* package extensively.

### Getting/Transforming Data and some Exploratory Data Analysis  

Based on the documentation, our desired target/response variable **classe** is a factor of 5 levels with values A to E, which implores us to consider this as a classification problem. Since there are 160 columns/variables to pick features from, prior to creating our training/validation sets from the given [training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), we will prune it of irrelevant features and those that have conceivably negligible contributions to the predictive power of our model. The transformation includes the following steps, carried out in order:

+ We load in the data as follows and normalize the NA values:
```{r load_data_1, cache=TRUE}
# Downloads the training and testing data
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", method = "curl", destfile = "pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", method = "curl", destfile = "pml-testing.csv")
# Loading and normalizing NA strings
df.train <- read.csv("pml-training.csv", header=TRUE, sep=",", na.strings=c("NA", "#DIV/0!"))
df.test <- read.csv("pml-testing.csv", header=TRUE, sep=",", na.strings=c("NA", "#DIV/0!"))
```
+ Cursory observation of the dataset had revealed the first seven columns are identity and timestamp related. These would have no actual predictive value
as they are not measurements for physical activity, and are thus removed. 
```{r transform_data_drop_id, cache=TRUE}
# Removal of 7 irrelevant id variables
col_idvar <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
df.train <- df.train [, ! names(df.train) %in% col_idvar, drop = FALSE]
```
+ Many values are observed to be "#DIV/0!" strings. Since the latter are derived from the actual measurements values using MS Excel worksheet formula functions (eg. for 12 "skewness" and 12 "kurtosis" variables describing the distribution of features rather than the features directly), as they are not primary sensor data, and thus would interfere with feature selection.
```{r transform_data_drop_excel, cache=TRUE}
# Removal of 24 MS Excel generated columns for skewness and kurtosis
col_excel <- grep(x=names(df.train), pattern="skewness|kurtosis")
df.train <- df.train[,-col_excel]
```
+ Many variables are observed to have a high proportion of "NA" values. Since this is a classification task, NA values contribute little, if any, information gain for each tree split and only serve to increase computational expense and also cause issues for some models in caret. Those columns that are over our threshold of 90% NA values are dropped.
```{r transform_data_drop_NA, cache=TRUE}
# Removal of variables composed of greater than 90% NA values
df.train[colMeans(is.na(df.train))>0.9] <- list(NULL)
```
+ Variables that have a higher than 90% correlation with another within the dataset are also removed, on the premise that because of this high correlation, simply using one of each pair of correlated variables will capture a large amount of variability in the response variable (hopefully!).
```{r transform_data_drop_correlated, cache=TRUE}
library(caret)
# Removal of highly correlated variables using cor()
correlated <- findCorrelation(cor(df.train[-dim(df.train)]), cutoff=0.9)
df.train <- df.train[,-correlated]
```
+ Use of the nearZeroVar function from *caret* indicated that there was nothing left to cull since there were no variables left that exhibit the two characteristics worth reducing (either too few unique values or variance in values).
```{r transform_data_zeroVar, cache=TRUE}
# Removal of variables with near zer
zVal <- nearZeroVar(df.train, saveMetrics=TRUE) # for metrics
print(zVal)
```
+ Our training and testing sets are then allocated as 70/30 from the training data given.
```{r transform_data_subset_traintest, cache=TRUE}
trainIndex <- createDataPartition(df.train$classe, p=0.7, list=FALSE)
training <- df.train[trainIndex,]
testing <- df.train[-trainIndex,]
```
Our training/testing sets thus consist of a shortlist of `r dim(training)[2]` predictors, split into `r dim(training)[1]` (training) and `r dim(testing)[1]` (testing) rows.

### Building preliminary decision tree to investigate the data set

Instead of directly training a model from the training set, we use a small subsample from the training set to investigate if heretofore unobserved characteristics of the dataset would favour or hinder various model types. They can also be used to cut down on the number of features in the final model, by checking each features' importance relative to the next. We set a seed from this point on to ensure reproducibility.
```{r prelim_model_investigation, cache=TRUE}
# Sets a seed
set.seed(55455)
# Using smaller subsample of 10% of the training data
subIndex1 <- createDataPartition(training$classe, p=0.1, list=FALSE)
sub1 <- training[subIndex1,]
trainingFinal <- training[-subIndex1,]
# Further split subsample into two parts for training/testing subsamples
subIndex2 <- createDataPartition(sub1$classe, p=0.5, list=FALSE)
sub2 <- sub1[-subIndex2,]
sub1 <- sub1[subIndex2,]
```
Using the subsamples we created, we build a basic classification tree to get a feel for the baseline accuracy, and perform 10 cross-validations on the model.
```{r rpart_baseline, cache=TRUE}
library(rpart)
# Setting xval=10 to perform 5 cross-validations
modRpart <- train(classe~., method="rpart", control=rpart.control(xval=5, minsplit=15, minbucket=5, cp=0.01, maxdepth=15), data=sub1)
#rpart(classe~., data=sub1, control=rpart.control(minsplit=30, minbucket=10, cp=0.01, maxdepth=10))
confusionMatrix(sub2$classe, predict(modRpart, sub2))
```
Our confusion matrix shows this model to have an accuracy not much better than a coin flip, even when allowing for relatively small splits/branches and a small complexity parameter of 0.01.

### Using a gradient boosted model to check variable importance

We proceed to use a different boosting model in order to improve on our decision tree. Using **gbm** we can check variable importance relative to other predictors out of the box. We do this along with 3-fold cross-validation (not too many to reduce chance of overfitting).
```{r fit_gbm, cache=TRUE}
library(gbm); library(survival); library(splines)
library(plyr); library(parallel)
fitControl <- trainControl(## 3-fold CV
                           method = "repeatedcv",
                           number = 3,
                           ## repeated 3 times
                           repeats = 3)
modGBM <- train(classe~., method="gbm", data=sub1, trControl=fitControl, verbose=FALSE)
confusionMatrix(sub2$classe, predict(modGBM, sub2))
v <- varImp(modGBM) ; plot(v, main="Variable Importance of GBM features")
```

Our plot of the model shows that much of the contribution to prediction is coming from the top few variables, and therefore surmise that we can cull our training data down to make use of even fewer features in the interests of parsimony. 

### Tuning our GBM for parsimony in final model fit

We select for the predictors that have an importance higher than 20, since we observed from the plot above a diminishing contribution beyond that point. Since we had used only a subsample (10% of the training data set) in order to determine the features to be used, we now fit our final model using the remaining 90% of the training data (previously stored as *trainingFinal*).
```{r cull_gbm_vimp, cache=TRUE}
# Cull using relative importance higher than 20
trainingCulled <- trainingFinal[, v$importance[,1]>=20]
names(trainingCulled[,-dim(trainingCulled)])
# Fit our final model with 3-fold cross-validation
modGBMculled <- train(classe~., method="gbm", data=trainingCulled, trControl = fitControl, verbose=FALSE)
prediction <- predict(modGBMculled, testing)
```

### Accuracy and Error
Our model with `r ncol(trainingCulled)-1` predictors (minus the target variable itself) can now be used on the testing data that we set aside previously in order to obtain the unbiased out-of-sample error rate. 
```{r error_calc, cache=TRUE}
confusionMatrix(testing$classe, prediction)
errorEst <- sum(prediction != testing$classe)/length(testing$classe)
```
Our estimate of the error rate is `r errorEst`. While we could have chosen to try another model (eg. "random forest"), this approach had given a relatively good accuracy and a relatively small number of predictors to use, and serves our parsimonious goal rather well.

```{r submit_code_only, echo=FALSE, eval=FALSE, cache=TRUE}
answers <- predict(modGBMculled, df.test)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```

```{r testcodeonly, echo=FALSE, eval=FALSE, cache=TRUE}
# RF
modRF <- train(classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=trainingCulled)
confusionMatrix(testing$classe, predict(modRF, testing))
answersRF <- predict(modRF, df.test)
# cross-validation for rpart 
printcp(modCart2)
modCart2$cptable[which.min(modCart2$cptable[,"xerror"]),"CP"]
plotcp(modCart2)
ptree <- prune(modCart2, cp=modCart2$cptable[which.min(modCart2$cptable[,"xerror"]),"CP"])
```

### Project Repo and References

+ All files and full code used are available from my [Github Project Repository](https://github.com/slothdev/pmlproj) (https://github.com/slothdev/pmlproj)
+ [Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har)
+ [Training data used](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)
+ [Testing Data used](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)
+ Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

All rights reserved.  

### END